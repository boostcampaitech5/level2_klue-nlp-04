import pickle as pickle
import os
import pandas as pd
import torch
import numpy as np
import time
import random
from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments, AdamW

from load_data import *
from metrics import *
from torch.utils.data import random_split
from CustomScheduler import CosineAnnealingWarmUpRestarts


def _getTrainerWithConfig(config):
    return TrainingArguments(
        fp16                            = config["train"]["fp16"],
        gradient_checkpointing          = config["train"]["gradient_checkpointing"],
        output_dir                      = config["train"]["output_dir"],
        save_total_limit                = int(config["train"]["save_total_limit"]),
        save_steps                      = int(config["train"]["save_steps"]),
        num_train_epochs                = int(config["train"]["num_train_epochs"]),
        learning_rate                   = float(config["train"]["learning_rate"]),
        per_device_eval_batch_size      = int(config["train"]["per_device_eval_batch_size"]),
        warmup_steps                    = int(config["train"]["warmup_steps"]),
        weight_decay                    = float(config["train"]["weight_decay"]),
        logging_dir                     = config["train"]["logging_dir"],
        logging_steps                   = int(config["train"]["logging_steps"]),
        eval_steps                      = int(config["train"]["eval_steps"]),
        evaluation_strategy             = config["train"]["evaluation_strategy"],
        load_best_model_at_end          = config["train"]["load_best_model_at_end"]
    )
  
  
def seed_everything(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

    
def train(args, config=None):
    seed_everything(42)

    MODEL_NAME = "klue/roberta-small"

    MODEL_NAME = args.model_name
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    # Entitiy Marker ì‚¬ìš©
    # added_special_tokens = ['[SE]', '[/SE]', '[OE]', '[/OE]']
    # added_token_num = tokenizer.add_special_tokens({'additional_special_tokens': added_special_tokens})


    # Data Load and Tokenizing
    # 1. Entity Marker ì‚¬ìš©
    # tokenized_total, total_label = tokenized_dataset("../dataset/train/train.csv", tokenizer, tokenizing_type="entity_marker", added_special_tokens)
    # 2. Base, typed Entity Marker Punct ì‚¬ìš©
    tokenized_total, total_label = tokenized_dataset("../dataset/train/train.csv", tokenizer, tokenizing_type="type_entity_marker_punct")


    # split dataset for pytorch.
    train_frac = 0.8
    RE_total_dataset = RE_Dataset(tokenized_total, total_label)
    RE_train_dataset, RE_val_dataset = random_split(RE_total_dataset, [int(len(RE_total_dataset)*train_frac), len(RE_total_dataset)-int(len(RE_total_dataset)*train_frac)])

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    print(device)
    
    # setting model hyperparameter
    model_config =  AutoConfig.from_pretrained(MODEL_NAME)
    model_config.num_labels = 30
    
    model =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)
    # Entity Marker ì‚¬ìš©
    # model.resize_token_embeddings(tokenizer.vocab_size + added_token_num) # ì¶”ê°€í•œ Special token ê°¯ìˆ˜ë§Œí¼ Embeddingì„ ëŠ˜ë ¤ì¤˜ì•¼í•¨
    print(model.config)
    model.parameters
    model.to(device)

    # optimizer and scheduler
    optimizers = AdamW(model.parameters(), lr=0)
    scheduler = CosineAnnealingWarmUpRestarts(optimizers, T_0=1000, T_mult=2, eta_max=3e-5,  T_up=500, gamma=0.5)

    training_args = TrainingArguments(
        fp16=True,                      # use 16-bit (mixed) precision to increase speed
        gradient_checkpointing=True,    # use gradient checkpointing to reduce memory usage
        # TODO:
        # adamw ëŒ€ì‹  adafactor, 8ë¹„íŠ¸ì•„ë‹´
        # ë°ì´í„°ë¡œë” pinê³¼ nun_worker ì„¤ì •í•˜ê¸°
        # ë°°ì¹˜ í¬ê¸° ëŠ˜ë¦¬ê¸°
        ##
        output_dir='./results',          # output directory
        save_total_limit=5,              # number of total save model.
        save_steps=500,                 # model saving step. ## check-pointê°€ ì—¬ê¸°ì•¼
        num_train_epochs=5,              # total number of training epochs
        learning_rate=5e-5,               # learning_rate
        per_device_train_batch_size=16,  # batch size per device during training
        per_device_eval_batch_size=16,   # batch size for evaluation
        warmup_steps=500,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
        logging_steps=100,              # log saving step.
        evaluation_strategy='steps', # evaluation strategy to adopt during training
                                     # `no`: No evaluation during training.
                                     # `steps`: Evaluate every `eval_steps`.
                                     # `epoch`: Evaluate every end of epoch.
        eval_steps = 500,            # evaluation step.

        load_best_model_at_end = True 
    )

    training_args = _getTrainerWithConfig(config) if config else training_args
    
    trainer = Trainer(
        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=RE_train_dataset,         # training dataset
        eval_dataset=RE_val_dataset,             # evaluation dataset ## ìˆ˜ì •
        compute_metrics=compute_metrics,        # define metrics function
        
        optimizers=(optimizers, scheduler)
        )

    # train model
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œì™€ ì´ë¦„ ì„¤ì •
    model_save_path = './best_model'
    import datetime
    KST = datetime.timezone(datetime.timedelta(hours=9))
    now = datetime.datetime.now(KST)
    now = now.strftime("%mM%dD%HH%MM")
    model_name = 'model_{}_{}'.format(MODEL_NAME, now)

    # ê²½ë¡œì™€ ì´ë¦„ì„ í•©ì³ì„œ ì™„ì „í•œ ê²½ë¡œ ìƒì„±
    model_path = os.path.join(model_save_path, model_name)

    # ëª¨ë¸ ì €ì¥ ê²½ë¡œì— í´ë”ê°€ ì—†ìœ¼ë©´ í´ë” ìƒì„±
    if not os.path.exists(model_save_path):
        os.makedirs(model_save_path)

    # ëª¨ë¸ ì €ì¥
    model.save_pretrained(model_path)
