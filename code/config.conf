[model]
; Model name from huggingface
model_name                  = klue/bert-base

[model.scheduler]
scheduler   = CosineAnnealingWarmUpRestarts
T_0         = 1000
T_mult      = 2
eta_max     = 3e-5
T_up        = 500
gamma       = 0.5

[dataset]
tokenizing_type         = type_entity_marker_punct
added_special_tokens    = ['[SE]', '[/SE]', '[OE]', '[/OE]']

[train]
; output directory
output_dir                  = ./results
; number of total save model.
save_total_limit            = 5
; model saving step.     
save_steps                  = 500
; total number of training epochs
num_train_epochs            = 5
; learning_rate
learning_rate               = 5e-5
; batch size per device during training
per_device_train_batch_size = 16
; batch size for evaluation
per_device_eval_batch_size  = 16
; number of warmup steps for learning rate scheduler
warmup_steps                = 100
; strength of weight decay
weight_decay                = 0.01
; directory for storing logs
logging_dir                 = ./logs
; log saving step.  
logging_steps               = 100
; evaluation strategy to adopt during training
    ; `no`: No evaluation during training.
    ; `steps`: Evaluate every `eval_steps`.
    ; `epoch`: Evaluate every end of epoch.
evaluation_strategy         = steps
; evaluation step.     
eval_steps                  = 500
; load best model at the end. 
load_best_model_at_end      = True
; use 16-bit (mixed) precision to increase speed
fp16                        = True
; use gradient checkpointing to reduce memory usage
gradient_checkpointing      = True

[inference]
; tokenizer name
tokenizer_name                  = klue/roberta-large
; saved model path
saved_model_path                = ./best_model
; output result file path
output_path                     = ./prediction/submission.csv

[preprocessing]
use_clean_duplicate = True

[sweep]
run_sweep                   = True
sweep_path                  = ./sweep.json
sweep_count                 = 5
